{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stable-alberta",
   "metadata": {},
   "source": [
    "### Training a Model Locally - A\n",
    "Mico Ellerich M. Comia\n",
    "\n",
    "This notebook trains a Sklearn's Logistic Regression model to predict a binary output given a multi-dimensional input. No hyperparameter optimizations were applied and as such, the default values were used as is.\n",
    "\n",
    "---\n",
    "\n",
    "- SELECT 2 MACHINE LEARNING ALGORITHMS \n",
    "- FOR EACH OF THE ALGORITHMS\n",
    "    - PERFORM TRAINING ON THE TRAINING DATASET\n",
    "    - EVALUATE ON THE VALIDATION DATASET\n",
    "    - TEST THE TRAINED MODEL ON THE TEST SET\n",
    "    - SAVE THE MODEL USING JOBLIB (OR ALTERNATIVE)\n",
    "- COMPARE THE “PERFORMANCE” OF THE 2 MODELS USING THE EVALUATION METRICS\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exciting-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-russell",
   "metadata": {},
   "source": [
    "### I. Import dataset splits\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-skill",
   "metadata": {},
   "source": [
    "First, we import the generated synthetic dataset from the previous notebook using Pandas' read_csv. This imports the CSV files as dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "roman-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  pd.read_csv('data/X_train.csv')\n",
    "X_test =  pd.read_csv('data/X_test.csv') \n",
    "X_val = pd.read_csv('data/X_val.csv') \n",
    "y_train =  pd.read_csv('data/y_train.csv') \n",
    "y_test =  pd.read_csv('data/y_test.csv') \n",
    "y_val = pd.read_csv('data/y_val.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-scotland",
   "metadata": {},
   "source": [
    "### II. Training the Logistic Regression model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-watson",
   "metadata": {},
   "source": [
    "#### A. Training on the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-diving",
   "metadata": {},
   "source": [
    "Using the fit method, we use the training set split to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-curtis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-pixel",
   "metadata": {},
   "source": [
    "Since we did not use automated hyperparameter turners or assigned different hyperparameter values, the default values for the model were used. The get_params method shows us these default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fossil-intro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.00,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.00,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accurate-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred_train =  logistic_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "secure-sharp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>89.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>86.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>94.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Scores\n",
       "Accuracy    89.33\n",
       "Precision   86.30\n",
       " Recall     94.57"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_train_scores = [metrics.accuracy_score(y_train, logistic_pred_train)*100,\n",
    "                     metrics.precision_score(y_train, logistic_pred_train)*100,\n",
    "                     metrics.recall_score(y_train, logistic_pred_train)*100] \n",
    "\n",
    "df_logi_train = pd.DataFrame(logi_train_scores, columns = ['Scores'], index = ['Accuracy', 'Precision', ' Recall'])\n",
    "df_logi_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-object",
   "metadata": {},
   "source": [
    "The trained model achieved an accuracy of 89.33% on the training set. We can check if our model is overfitting or underfitting once we compare these values with the validiation and test set scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-spray",
   "metadata": {},
   "source": [
    "---\n",
    "#### B. Evaluate on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-clock",
   "metadata": {},
   "source": [
    "To evaluate the performance of our model, we use the accuracy, precision, and recall metrics. A higher value for these metrics are desirable. The evaluation steps are similar for both the validation and test sets. Ideally, we use the validation set when we're performing cross validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suffering-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred_val =  logistic_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hydraulic-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>90.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>85.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>96.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Scores\n",
       "Accuracy    90.50\n",
       "Precision   85.19\n",
       " Recall     96.84"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_val_scores = [metrics.accuracy_score(y_val, logistic_pred_val)*100,\n",
    "                   metrics.precision_score(y_val, logistic_pred_val)*100,\n",
    "                   metrics.recall_score(y_val, logistic_pred_val)*100] \n",
    "\n",
    "df_logi_val = pd.DataFrame(logi_val_scores, columns = ['Scores'], index = ['Accuracy', 'Precision', ' Recall'])\n",
    "df_logi_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-embassy",
   "metadata": {},
   "source": [
    "For our validation set, we can see that the trained model attained respectable scores, garnering an accuracy of 90.50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-circular",
   "metadata": {},
   "source": [
    "---\n",
    "#### C. Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wrapped-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred_test =  logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "integrated-corpus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>88.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>92.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Scores\n",
       "Accuracy    88.50\n",
       "Precision   84.00\n",
       " Recall     92.31"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_test_scores = [metrics.accuracy_score(y_test, logistic_pred_test)*100,\n",
    "                    metrics.precision_score(y_test, logistic_pred_test)*100,\n",
    "                    metrics.recall_score(y_test, logistic_pred_test)*100] \n",
    "\n",
    "df_logi_test = pd.DataFrame(logi_test_scores, columns = ['Scores'], index = ['Accuracy', 'Precision', ' Recall'])\n",
    "df_logi_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-bedroom",
   "metadata": {},
   "source": [
    "The difference between the three sets are small. From this, we can assume that the model is neither underfitting nor overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-energy",
   "metadata": {},
   "source": [
    "---\n",
    "#### D. Saving metrics and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-touch",
   "metadata": {},
   "source": [
    "For future use and reference, we save the scores of the model and the model itself. For the metrics, we have concatenated the different scores into a single dataframe and exported it as CSV with a timestamp. We also use the store magic so that we can access it in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "rational-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the current time to serve as timestamps\n",
    "timestr = time.strftime(\"%m%d-%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logi_scores = pd.concat([df_logi_train, df_logi_val, df_logi_test], axis = 1)\n",
    "df_logi_scores.columns = ['Training','Validation','Test']\n",
    "df_logi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_filename = 'model/results/logistic_' + timestr + '.csv'\n",
    "df_logi_scores.to_csv(metrics_filename, index = False)\n",
    "%store df_logi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-testament",
   "metadata": {},
   "source": [
    "Likewise, we do the same for the model itself, attaching the timestamp for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlike-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/logistic_0528-1815.sav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model saving\n",
    "model_filename = 'model/logistic_' + timestr + '.sav'\n",
    "joblib.dump(logistic_model, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
